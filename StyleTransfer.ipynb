{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066effac-0ffe-4247-aa39-1658fb1b76bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms,models\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms.functional import invert\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b94ce606-4df5-42c8-a94a-711922d5b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferDataset(Dataset):\n",
    "    def __init__(self, gothic_dir, handwriting_dir, transform=None):\n",
    "        self.gothic_images = sorted([os.path.join(gothic_dir, img) for img in os.listdir(gothic_dir)])\n",
    "        self.handwriting_images = sorted([os.path.join(handwriting_dir, img) for img in os.listdir(handwriting_dir)])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gothic_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gothic_image = Image.open(self.gothic_images[idx]).convert(\"RGB\")\n",
    "        handwriting_image = Image.open(self.handwriting_images[idx]).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            gothic_image = self.transform(gothic_image)\n",
    "            handwriting_image = self.transform(handwriting_image)\n",
    "        \n",
    "        return gothic_image, handwriting_image\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebdad99e-7e51-4471-9993-873d2e522475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 설정\n",
    "gothic_dir = '../data/FONT_NONE_CLASS_GODIC'\n",
    "handwriting_dir = '../data/FONT_NONE_CLASS'\n",
    "\n",
    "# 데이터셋 및 데이터로더 초기화\n",
    "dataset = StyleTransferDataset(gothic_dir, handwriting_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f8ce823-23e6-48f2-b45f-831ac2afb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # VGG19의 특성 추출 레이어\n",
    "        self.vgg_features = models.vgg19(pretrained=True).features[:21]  # Conv4_1까지 사용\n",
    "        for param in self.vgg_features.parameters():\n",
    "            param.requires_grad = False  # VGG19의 가중치는 고정\n",
    "        \n",
    "        # 추가적인 학습 가능한 층들\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()  # 출력 범위를 [-1, 1]로 설정\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.vgg_features(x)  # VGG19의 특성 추출\n",
    "        x = self.deconv_layers(x)  # 학습 가능한 층을 통과\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4733f350-9d48-4365-b7e7-8780142b00e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lovee\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lovee\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 생성자와 판별자 초기화\n",
    "generator = Generator().cuda()\n",
    "discriminator = Discriminator().cuda()\n",
    "\n",
    "# 손실 함수\n",
    "adversarial_loss = nn.BCELoss().cuda()\n",
    "mse_loss = nn.MSELoss().cuda()\n",
    "\n",
    "# 옵티마이저\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c2f0284-78ec-4b96-92a9-cf7d4c099241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def invert_colors(image):\n",
    "    return 1 - image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c32179-16e1-4640-bc9a-a9eeeb574900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generated_images(images, num_images=128):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Images\")\n",
    "    images = vutils.make_grid(images[:num_images], padding=2, normalize=True)\n",
    "    images = np.transpose(images.cpu(), (1, 2, 0))\n",
    "    plt.imshow(images)\n",
    "    plt.show()\n",
    "\n",
    "def save_generated_images(images, num_images, epoch, idx):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Images\")\n",
    "    images = vutils.make_grid(images[:num_images], padding=2, normalize=True)\n",
    "    images = np.transpose(images.cpu(), (1, 2, 0))\n",
    "    # plt.imshow(images)\n",
    "    fname = '../data/RESULT_SAVE/'+str(epoch)+'_'+str(idx)+'.jpg'\n",
    "    plt.imsave(fname, images.numpy())\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e7b0e-bc0d-4233-b717-48ce09c48873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ef3f8-0d6b-40f8-a6b0-b46443bda64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abca4b-a065-4c06-abcc-44709edfe6f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][3300/10000] Loss_D: 0.5713 Loss_G: 0.9666\n",
      "[0/10][6150/10000] Loss_D: 0.5512 Loss_G: 1.1469\n",
      "[0/10][50/10000] Loss_D: 0.1749 Loss_G: 2.1540\n",
      "[0/10][350/10000] Loss_D: 0.0902 Loss_G: 3.8696\n",
      "[0/10][9600/10000] Loss_D: 0.0751 Loss_G: 4.1107\n",
      "[0/10][6450/10000] Loss_D: 0.2386 Loss_G: 2.5111\n",
      "[0/10][4800/10000] Loss_D: 0.0012 Loss_G: 7.1000\n",
      "[0/10][6900/10000] Loss_D: 0.0050 Loss_G: 5.8397\n",
      "[0/10][2250/10000] Loss_D: 0.0133 Loss_G: 5.2150\n",
      "[0/10][3600/10000] Loss_D: 0.0050 Loss_G: 5.0590\n",
      "[0/10][3800/10000] Loss_D: 0.0009 Loss_G: 6.9889\n",
      "[0/10][7600/10000] Loss_D: 0.0010 Loss_G: 7.1129\n",
      "[0/10][6950/10000] Loss_D: 0.0006 Loss_G: 7.4407\n",
      "[0/10][4250/10000] Loss_D: 0.0005 Loss_G: 8.0218\n",
      "[0/10][4750/10000] Loss_D: 0.0004 Loss_G: 8.0099\n",
      "[0/10][2550/10000] Loss_D: 0.0004 Loss_G: 8.2482\n",
      "[0/10][7500/10000] Loss_D: 0.0004 Loss_G: 8.0707\n",
      "[0/10][4600/10000] Loss_D: 0.0003 Loss_G: 8.2341\n",
      "[0/10][1950/10000] Loss_D: 0.0003 Loss_G: 8.4232\n",
      "[0/10][0/10000] Loss_D: 0.0004 Loss_G: 8.3467\n",
      "[0/10][8200/10000] Loss_D: 0.0002 Loss_G: 8.5250\n",
      "[0/10][6250/10000] Loss_D: 0.0002 Loss_G: 8.4819\n",
      "[0/10][7700/10000] Loss_D: 0.0002 Loss_G: 8.7584\n",
      "[0/10][1650/10000] Loss_D: 0.0002 Loss_G: 8.7360\n",
      "[0/10][8400/10000] Loss_D: 0.0002 Loss_G: 8.9280\n",
      "[0/10][1200/10000] Loss_D: 0.0002 Loss_G: 8.9733\n",
      "[0/10][7850/10000] Loss_D: 0.0002 Loss_G: 9.0011\n",
      "[0/10][6800/10000] Loss_D: 0.0005 Loss_G: 8.8524\n",
      "[0/10][1100/10000] Loss_D: 0.0001 Loss_G: 8.9445\n",
      "[0/10][2650/10000] Loss_D: 0.0001 Loss_G: 9.0099\n",
      "[0/10][7800/10000] Loss_D: 0.0001 Loss_G: 9.1678\n",
      "[0/10][7350/10000] Loss_D: 0.0001 Loss_G: 9.2945\n",
      "[0/10][1300/10000] Loss_D: 0.0001 Loss_G: 9.1747\n",
      "[0/10][7900/10000] Loss_D: 0.0001 Loss_G: 9.2905\n",
      "[0/10][3050/10000] Loss_D: 0.0001 Loss_G: 9.2826\n",
      "[0/10][900/10000] Loss_D: 0.0001 Loss_G: 9.3296\n",
      "[0/10][2600/10000] Loss_D: 0.0001 Loss_G: 9.5149\n",
      "[0/10][3500/10000] Loss_D: 0.0001 Loss_G: 9.4984\n",
      "[0/10][9150/10000] Loss_D: 0.0001 Loss_G: 9.5962\n",
      "[0/10][1000/10000] Loss_D: 0.0001 Loss_G: 9.7611\n",
      "[0/10][5450/10000] Loss_D: 0.0001 Loss_G: 9.7447\n",
      "[0/10][6750/10000] Loss_D: 0.0001 Loss_G: 9.7289\n",
      "[0/10][5350/10000] Loss_D: 0.0001 Loss_G: 9.7273\n",
      "[0/10][3900/10000] Loss_D: 0.0001 Loss_G: 9.7718\n",
      "[0/10][6050/10000] Loss_D: 0.0001 Loss_G: 9.7799\n",
      "[0/10][2350/10000] Loss_D: 0.0001 Loss_G: 9.9248\n",
      "[0/10][4650/10000] Loss_D: 0.0001 Loss_G: 9.9068\n",
      "[0/10][3250/10000] Loss_D: 0.0001 Loss_G: 9.9316\n",
      "[0/10][9000/10000] Loss_D: 0.0001 Loss_G: 10.0513\n",
      "[0/10][8600/10000] Loss_D: 0.0001 Loss_G: 10.1492\n",
      "[0/10][4450/10000] Loss_D: 0.0001 Loss_G: 10.0392\n",
      "[0/10][8900/10000] Loss_D: 0.0000 Loss_G: 10.0997\n",
      "[0/10][1500/10000] Loss_D: 0.0000 Loss_G: 10.2140\n",
      "[0/10][5200/10000] Loss_D: 0.0000 Loss_G: 10.1625\n",
      "[0/10][6500/10000] Loss_D: 0.0000 Loss_G: 10.2662\n",
      "[0/10][5300/10000] Loss_D: 0.0000 Loss_G: 10.3112\n",
      "[0/10][2400/10000] Loss_D: 0.0000 Loss_G: 10.2086\n",
      "[0/10][7750/10000] Loss_D: 0.0000 Loss_G: 10.2397\n",
      "[0/10][150/10000] Loss_D: 0.0000 Loss_G: 10.3395\n",
      "[0/10][5000/10000] Loss_D: 0.0001 Loss_G: 9.6064\n",
      "[0/10][7650/10000] Loss_D: 0.0001 Loss_G: 9.8448\n",
      "[0/10][6200/10000] Loss_D: 0.0001 Loss_G: 9.8737\n",
      "[0/10][650/10000] Loss_D: 0.0008 Loss_G: 8.3260\n",
      "[0/10][7300/10000] Loss_D: 0.0078 Loss_G: 8.6625\n",
      "[0/10][9050/10000] Loss_D: 0.0000 Loss_G: 11.2268\n",
      "[0/10][600/10000] Loss_D: 0.0000 Loss_G: 11.2231\n",
      "[0/10][2450/10000] Loss_D: 0.0000 Loss_G: 11.1913\n",
      "[0/10][3550/10000] Loss_D: 0.0000 Loss_G: 11.3477\n",
      "[0/10][4500/10000] Loss_D: 0.0000 Loss_G: 11.4179\n",
      "[0/10][200/10000] Loss_D: 0.0000 Loss_G: 11.3618\n",
      "[0/10][500/10000] Loss_D: 0.0000 Loss_G: 11.4188\n",
      "[0/10][7550/10000] Loss_D: 0.0000 Loss_G: 11.4861\n",
      "[0/10][6350/10000] Loss_D: 0.0000 Loss_G: 11.6491\n",
      "[0/10][8350/10000] Loss_D: 0.0001 Loss_G: 11.3655\n",
      "[0/10][800/10000] Loss_D: 0.0000 Loss_G: 11.6789\n",
      "[0/10][450/10000] Loss_D: 0.0000 Loss_G: 11.4365\n",
      "[0/10][100/10000] Loss_D: 0.0000 Loss_G: 11.5681\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 훈련 루프\n",
    "num_epochs = 10\n",
    "output_dir = 'output_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 데이터 인덱스 셔플링\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "    \n",
    "    for idx in indices:\n",
    "        gothic, handwriting = dataset[idx]\n",
    "        gothic, handwriting = gothic.to(device).unsqueeze(0), handwriting.to(device).unsqueeze(0)\n",
    "        batch_size = gothic.size(0)\n",
    "\n",
    "        # 판별자 출력 크기에 맞춰 목표 텐서 크기 조정\n",
    "        valid = torch.ones(discriminator(gothic).size(), requires_grad=False).to(device)\n",
    "        fake = torch.zeros(discriminator(gothic).size(), requires_grad=False).to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        generated_images = generator(gothic)\n",
    "\n",
    "        # Adversarial loss\n",
    "        g_adv_loss = adversarial_loss(discriminator(generated_images), valid)\n",
    "\n",
    "        # Content loss\n",
    "        content_loss = mse_loss(generated_images, handwriting)\n",
    "\n",
    "        # Total loss\n",
    "        g_loss = g_adv_loss + content_loss\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        real_loss = adversarial_loss(discriminator(handwriting), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(generated_images.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if idx % 50 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f'\n",
    "                  % (epoch, num_epochs, idx, len(dataset), d_loss.item(), g_loss.item()))\n",
    "            # fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
    "            fake_images = generator(gothic)\n",
    "            save_generated_images(gothic, 128, epoch=epoch, idx=idx)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1167397-cb84-4319-b43d-45e5f66d9b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2edb75-3e7a-4e5f-b598-3ebe2c6130c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002fe0e-4505-42ea-9aed-96fcb7bf52b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505c627-8116-4db0-b562-6291500ba3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45af1c-ee42-41c1-9705-21453b7e17f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
